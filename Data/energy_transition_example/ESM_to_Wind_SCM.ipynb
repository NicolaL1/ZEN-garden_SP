{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T17:56:57.728390Z",
     "start_time": "2025-05-04T17:56:57.454297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#this section saves the capacity and capacity addition from the ESM and aggregates them for the specified locations\n",
    "#works for all SCMs\n",
    "from zen_garden.postprocess.results import Results\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "dataset_name = \"esm_pv_wind\" #adjust dataset_name for your dataset\n",
    "base_path = Path(r\".\\outputs\")\n",
    "results_path = base_path / dataset_name\n",
    "output_dir = Path(\"./csv_output\") / dataset_name\n",
    "\n",
    "selected_technologies = {\"heat_pump\", \"photovoltaics\", \"wind_offshore\", \"wind_onshore\"} #adjust relevant selected technologies\n",
    "specific_locations = {\"DE\", \"CH\"} #specify locations that are not summed into ROE (rest of europe)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load Results\n",
    "res = Results(results_path)\n",
    "\n",
    "capacity = res.get_full_ts(\"capacity\").reset_index()\n",
    "capacity_addition = res.get_full_ts(\"capacity_addition\").reset_index()\n",
    "\n",
    "# Filter Technologies\n",
    "capacity = capacity[capacity[\"technology\"].isin(selected_technologies)]\n",
    "capacity_addition = capacity_addition[capacity_addition[\"technology\"].isin(selected_technologies)]\n",
    "\n",
    "# Save Filtered CSVs\n",
    "capacity.round(4).to_csv(output_dir / \"capacity.csv\", index=False)\n",
    "capacity_addition.round(4).to_csv(output_dir / \"capacity_addition.csv\", index=False)\n",
    "\n",
    "# Aggregation Setup\n",
    "all_locations = set(capacity[\"location\"].unique())\n",
    "roe_locations = all_locations - specific_locations\n",
    "\n",
    "def aggregate_by_location(df, location_set, location_name):\n",
    "    return (\n",
    "        df[df[\"location\"].isin(location_set)]\n",
    "        .groupby([\"technology\", \"capacity_type\"])\n",
    "        .sum(numeric_only=True)\n",
    "        .assign(location=location_name)\n",
    "    )\n",
    "\n",
    "# Aggregate Data\n",
    "capacity_aggregated = pd.concat([\n",
    "    aggregate_by_location(capacity, {\"DE\"}, \"DE\"),\n",
    "    aggregate_by_location(capacity, {\"CH\"}, \"CH\"),\n",
    "    aggregate_by_location(capacity, roe_locations, \"ROE\")\n",
    "]).reset_index()\n",
    "\n",
    "capacity_addition_aggregated = pd.concat([\n",
    "    aggregate_by_location(capacity_addition, {\"DE\"}, \"DE\"),\n",
    "    aggregate_by_location(capacity_addition, {\"CH\"}, \"CH\"),\n",
    "    aggregate_by_location(capacity_addition, roe_locations, \"ROE\")\n",
    "]).reset_index()\n",
    "\n",
    "# Save Aggregated CSVs\n",
    "capacity_aggregated.round(4).to_csv(output_dir / \"capacity_aggregated_by_location.csv\", index=False)\n",
    "capacity_addition_aggregated.round(4).to_csv(output_dir / \"capacity_addition_aggregated_by_location.csv\", index=False)\n",
    "\n",
    "print(f\"CSV files for dataset '{dataset_name}' saved in: {output_dir}\")\n"
   ],
   "id": "32e148c14602dc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files for dataset 'esm_pv_wind' saved in: csv_output\\esm_pv_wind\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T17:56:57.755601Z",
     "start_time": "2025-05-04T17:56:57.733433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# rename_year_columns.py\n",
    "# This script updates aggregated CSV column headers with actual years from system configuration.\n",
    "# Assumes dataset_name, output_dir, and results_path are already defined.\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load system.json\n",
    "system_path = results_path / \"system.json\"\n",
    "with open(system_path, \"r\") as f:\n",
    "    system_config = json.load(f)\n",
    "\n",
    "reference_year = system_config[\"reference_year\"]\n",
    "interval = system_config[\"interval_between_years\"]\n",
    "optimized_years = system_config[\"optimized_years\"]\n",
    "\n",
    "# Create rename mapping\n",
    "year_labels = [str(reference_year + i * interval) for i in range(optimized_years)]\n",
    "rename_columns = {str(i): year_labels[i] for i in range(len(year_labels))}\n",
    "\n",
    "# Function to rename year columns in a CSV\n",
    "def rename_year_columns(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.rename(columns=rename_columns)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Renamed year columns in: {csv_path.name}\")\n",
    "\n",
    "# Apply to aggregated CSV files\n",
    "rename_year_columns(output_dir / \"capacity_aggregated_by_location.csv\")\n",
    "rename_year_columns(output_dir / \"capacity_addition_aggregated_by_location.csv\")"
   ],
   "id": "7023437a85eab05b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed year columns in: capacity_aggregated_by_location.csv\n",
      "Renamed year columns in: capacity_addition_aggregated_by_location.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T17:56:57.826600Z",
     "start_time": "2025-05-04T17:56:57.798414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#specific for wind (write values to demand_yearly_variation.csv)\n",
    "#import pandas as pd\n",
    "import shutil\n",
    "#from pathlib import Path\n",
    "#import json\n",
    "\n",
    "# CONFIGURATION\n",
    "#dataset_name = \"energy_transition_ref_2021\"\n",
    "\n",
    "# Base paths\n",
    "#base_path = Path(r\".\\outputs\")\n",
    "#output_dir = Path(\"./CSV_output\") / dataset_name\n",
    "esm_capacity_path = output_dir / \"capacity_addition_aggregated_by_location.csv\"\n",
    "\n",
    "original_dir = Path(r\"C:\\Users\\nicol\\OneDrive\\Dokumente\\ZEN-garden_SP\\Data_Wind\\Data_WT\")\n",
    "new_dir = Path(r\"C:\\Users\\nicol\\OneDrive\\Dokumente\\ZEN-garden_SP\\Data_Wind\\Data_WT_new\")\n",
    "original_dyv_path = original_dir / \"set_carriers\" / \"Turbine\" / \"demand_yearly_variation.csv\"\n",
    "target_dyv_path = new_dir / \"set_carriers\" / \"Turbine\" / \"demand_yearly_variation.csv\"\n",
    "\n",
    "# --- Step 1: Clone the original directory ---\n",
    "if new_dir.exists():\n",
    "    print(f\"Directory already exists: {new_dir}\")\n",
    "else:\n",
    "    shutil.copytree(original_dir, new_dir)\n",
    "    print(f\"Created clone of wind model input: {new_dir}\")\n",
    "\n",
    "# --- Step 2: Load year labels from system.json ---\n",
    "system_path = base_path.parent / dataset_name / \"system.json\"\n",
    "with open(system_path, \"r\") as f:\n",
    "    system_config = json.load(f)\n",
    "\n",
    "reference_year = system_config[\"reference_year\"]\n",
    "interval = system_config[\"interval_between_years\"]\n",
    "optimized_years = system_config[\"optimized_years\"]\n",
    "year_labels = [str(reference_year + i * interval) for i in range(optimized_years)]\n",
    "\n",
    "# --- Step 3: Load original demand_yearly_variation to preserve all nodes ---\n",
    "df_existing = pd.read_csv(original_dyv_path)\n",
    "\n",
    "# --- Step 4: Prepare updated wind data from ESM ---\n",
    "country_map = {\n",
    "    \"DE\": \"DEU\",\n",
    "    \"CH\": \"CHE\",\n",
    "    \"AT\": \"AUT\",\n",
    "    \"NL\": \"NLD\",\n",
    "    \"ROE\": \"ROE\",\n",
    "    \"ROW\": \"ROW\",\n",
    "}\n",
    "\n",
    "df = pd.read_csv(esm_capacity_path)\n",
    "df_wind = df[df[\"technology\"].isin([\"wind_onshore\", \"wind_offshore\"])]\n",
    "df_grouped = df_wind.groupby([\"location\"]).sum(numeric_only=True).reset_index()\n",
    "df_grouped[\"node\"] = df_grouped[\"location\"].map(country_map)\n",
    "df_grouped = df_grouped[df_grouped[\"node\"].notna()].drop(columns=[\"location\"])\n",
    "\n",
    "# Transpose to match demand_yearly_variation structure\n",
    "df_new = df_grouped.set_index(\"node\").T\n",
    "df_new.index.name = \"year\"\n",
    "df_new = df_new.reset_index()\n",
    "\n",
    "# Fix year column using actual labels from system.json\n",
    "df_new[\"year\"] = year_labels[:len(df_new)]\n",
    "df_new[\"year\"] = df_new[\"year\"].astype(str)\n",
    "df_existing[\"year\"] = df_existing[\"year\"].astype(str)\n",
    "\n",
    "# Set year as index for merging\n",
    "df_new = df_new.set_index(\"year\")\n",
    "df_existing = df_existing.set_index(\"year\")\n",
    "\n",
    "# --- Step 5: Merge values only for matching columns ---\n",
    "updated_cols = [col for col in df_new.columns if col in df_existing.columns]\n",
    "\n",
    "for col in updated_cols:\n",
    "    df_existing[col] = df_new[col].combine_first(df_existing[col])\n",
    "\n",
    "# --- Step 6: Save final file ---\n",
    "df_result = df_existing.reset_index()\n",
    "df_result.to_csv(target_dyv_path, index=False)\n",
    "print(f\"Final demand_yearly_variation.csv written to:\\n{target_dyv_path}\")\n"
   ],
   "id": "e6954ffb0eaed92c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: C:\\Users\\nicol\\OneDrive\\Dokumente\\ZEN-garden_SP\\Data_Wind\\Data_WT_new\n",
      "Final demand_yearly_variation.csv written to:\n",
      "C:\\Users\\nicol\\OneDrive\\Dokumente\\ZEN-garden_SP\\Data_Wind\\Data_WT_new\\set_carriers\\Turbine\\demand_yearly_variation.csv\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
